{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "epxjzyyhoa4rwpyu77v6",
   "authorId": "127400340872",
   "authorName": "PRASHMED",
   "authorEmail": "prash.medirattaa@snowflake.com",
   "sessionId": "4783056c-dfe1-4b51-9ab5-9567b86372ba",
   "lastEditTime": 1741822884077
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5dbbeb9-4b6d-4bbc-a064-a2b0354f9959",
   "metadata": {
    "name": "End_2_End_ML",
    "collapsed": false
   },
   "source": "## End to End ML In Snowflake \n\n#### Data Analysis & Preparation\n\n- Perform exploratory data analysis (EDA) on transaction data\n- Engineer fraud detection features using Snowpark & Cortex\n- Utilize Feature Store to:\n    - Track engineered fraud indicators\n    - Store feature definitions for reproducible fraud detection signals\n\n#### Model Development\n\n- Train multiple fraud detection models:\n    - SnowML XGBoost with tree booster\n    - SnowML XGBoost with linear booster\n    - Multiple scikit-learn classification models\n\n- Register all models in Snowflake model registry\n- Explore registry capabilities:\n    - Metadata tracking\n    - Inference for fraud predictions\n    - Explainability of fraud determinations\n\n#### Model Evaluation & Monitoring\n\n- Configure Model Monitor to track 1 year of fraud predictions against confirmed fraud cases\n- Compute key performance metrics:\n    - F1 score (balance between precision and recall)\n    - Precision (minimize false positives)\n    -  Recall (capture all actual fraud)\n- Analyze model drift (track changes in fraud detection patterns day-to-day)\n-  Compare models side-by-side to determine best production candidate\n- Identify and address data quality issues in fraud detection pipeline\n\n\n#### Inference \n    - Inference on Warehouse \n    - Inference on SPCS\n\n#### Lineage & Governance\n\n- Track comprehensive data and model lineage throughout the fraud detection system\n- Maintain visibility into:\n    - Origin of data used for computed fraud indicators\n    - Datasets used for fraud model training\n    - Available fraud detection model versions under monitoring\n\n#### Deployment\n\n- Create Streamlit application for fraud analysts to make informed decisions about flagged transactions leveraging cortex analyst on predictions\n"
  },
  {
   "cell_type": "code",
   "id": "6fea6e9c-2405-4d58-81a9-89a1ef647660",
   "metadata": {
    "language": "sql",
    "name": "code_dump"
   },
   "outputs": [],
   "source": "-- CREATE OR REPLACE NETWORK RULE allow_all_rule\n-- MODE = 'EGRESS'\n-- TYPE = 'HOST_PORT'\n-- VALUE_LIST = ('0.0.0.0:443','0.0.0.0:80');\n-- CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION allow_all_integration\n-- ALLOWED_NETWORK_RULES = (allow_all_rule)\n-- ENABLED = true;\n\n-- GRANT USAGE ON INTEGRATION allow_all_integration TO ROLE PUBLIC;\n--!pip install shap",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea0c876b-d5f9-4c61-855d-111e111ed26d",
   "metadata": {
    "language": "python",
    "name": "Importing_libraries"
   },
   "outputs": [],
   "source": "# Standard Python Libraries\nimport sys\nimport json\nimport warnings\nfrom datetime import timedelta\n\n# Data Manipulation and Analysis\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\n\n# Snowpark Core\nfrom snowflake.snowpark import Session, DataFrame\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\nfrom snowflake.snowpark.functions import (sproc, col, dayname, \n                              to_timestamp,min, max,split\n)\n\n\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.window import Window\n\n# Snowpark ML\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\nfrom snowflake.ml.registry import Registry\n\n# Snowflake Feature Store\nfrom snowflake.ml.feature_store import (\n    FeatureStore, FeatureView, Entity, CreationMode, setup_feature_store\n)\n\n# Snowflake Task API\nfrom snowflake.core import Root\nfrom snowflake.core.database import Database\nfrom snowflake.core.schema import Schema\nfrom snowflake.core.warehouse import Warehouse\nfrom snowflake.core.task import StoredProcedureCall\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\nfrom snowflake.core._common import CreateMode\n\n# Streamlit\nimport streamlit as st\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cfd9091-b782-450c-bbf2-b5a129b115af",
   "metadata": {
    "language": "python",
    "name": "session_start",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3a62b34-28b6-429b-90c8-ffc9e6f6a2ab",
   "metadata": {
    "name": "Data_Generation",
    "collapsed": false
   },
   "source": "\n### Data Generation for Fraud Detection\n\n*   **Purpose:** Generates synthetic data for fraud detection.\n*   **Tables:** Creates two tables: `transactions` and `customer_complaints`.\n*   **`transactions` Table:**\n    *   Simulates 10,000 financial transactions.\n    *   Includes randomized transaction amounts, merchant categories, device types, locations, and IP addresses.\n    *   Marks 15% of transactions as fraudulent.\n*   **`customer_complaints` Table:**\n    *   Simulates 2,000 customer-reported issues.\n    *   Includes complaint text, sentiment scores, and keywords.\n*   **Relationships:**\n    *   Tables are linked by `customer_id`.\n    *   Fraud keywords in complaints map to fraudulent transactions.\n*   **Use Cases:** Supports machine learning, anomaly detection, and temporal analysis for fraud detection.\n\n\n[Data Generation Script](src/datagneration.sql)"
  },
  {
   "cell_type": "code",
   "id": "80563a67-0bba-45ef-8e59-fa9deb7e71fc",
   "metadata": {
    "language": "sql",
    "name": "transaction_df",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from transactions limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84f93abc-053b-417b-9317-7c3db63e9776",
   "metadata": {
    "language": "sql",
    "name": "customer_df",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from customer_complaints limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a319d9c5-e621-4e8d-87bc-ba72f7203638",
   "metadata": {
    "language": "python",
    "name": "Merge_two_datasets",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create the table using SQL\nsession.sql(\"\"\"\nCREATE OR REPLACE TABLE fraud_analysis AS\nSELECT \n    t.transaction_id, \n    t.customer_id, \n    t.transaction_amount, \n    t.is_fraud, \n    t.merchant_category,\n    t.device_type,\n    t.location,\n    t.transaction_time,\n    c.complaint_text, \n    c.keywords,\n    c.complaint_time\nFROM transactions t\nLEFT JOIN customer_complaints c\nON t.customer_id = c.customer_id\n\"\"\").collect()\n\n# Create a Snowpark DataFrame from the newly created table\ndf = session.table(\"fraud_analysis\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1043d068-7dee-49a2-9f7f-194d901858d2",
   "metadata": {
    "language": "sql",
    "name": "fA_list",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from fraud_analysis limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a37846b-9160-474c-bd79-0abe012086f4",
   "metadata": {
    "language": "python",
    "name": "EDA_CODE",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col, count\n\n# 1. Basic Data Overview\ndef basic_data_overview(df):\n    print(\"Dataset Shape:\")\n    row_count = df.count()\n    col_count = len(df.columns)\n    print(f\"Rows: {row_count}, Columns: {col_count}\")\n    \n    print(\"\\nColumn Names:\")\n    print(df.columns)\n    \n    print(\"\\nData Types:\")\n    for col_name, col_type in df.dtypes:\n        print(f\"{col_name}: {col_type}\")\n    \n    # Sample data\n    print(\"\\nSample Data:\")\n    print(df.limit(5).toPandas())\n    \n    return row_count\n\n# 2. Missing Values Analysis\ndef missing_values_analysis(df, row_count):\n    print(\"Missing Values Analysis:\")\n    \n    missing_counts = {}\n    for column in df.columns:\n        missing_count = df.filter(col(column).isNull()).count()\n        missing_counts[column] = missing_count\n    \n    missing_df = pd.DataFrame({\n        'Column': list(missing_counts.keys()),\n        'Missing Count': list(missing_counts.values())\n    })\n    missing_df['Missing Percentage'] = (missing_df['Missing Count'] / row_count) * 100\n    \n    return missing_df.sort_values('Missing Percentage', ascending=False)\n\n# 3. Categorical Features Analysis\ndef categorical_features_analysis(df, row_count):\n    print(\"Categorical Features Analysis:\")\n    \n    # Use uppercase column names to match your dataset\n    categorical_cols = [\"MERCHANT_CATEGORY\", \"DEVICE_TYPE\", \"LOCATION\"]\n    \n    categorical_stats = {}\n    for cat_col in categorical_cols:\n        # Distribution by category\n        cat_dist = df.groupBy(cat_col).count().withColumn(\n            \"percentage\", col(\"count\") / row_count * 100\n        ).orderBy(\"count\", ascending=False).toPandas()\n        \n        # Fraud rate by category\n        fraud_by_cat = df.groupBy(cat_col, \"IS_FRAUD\").count().toPandas()\n        \n        categorical_stats[cat_col] = {\n            \"distribution\": cat_dist,\n            \"fraud_rate\": fraud_by_cat\n        }\n    \n    return categorical_stats\n\n# 4. Correlation Analysis\ndef correlation_analysis(df):\n    print(\"Correlation Analysis:\")\n    \n    # Convert the Snowpark DataFrame to pandas for correlation analysis\n    # This will only work for a reasonable sized dataset\n    # For large datasets, you would need to compute correlations in Snowflake\n    try:\n        # Use uppercase column names\n        pdf = df.select(\"TRANSACTION_AMOUNT\", \"IS_FRAUD\").toPandas()\n        pdf[\"IS_FRAUD\"] = pdf[\"IS_FRAUD\"].astype(int)\n        \n        # Add other numeric columns as needed\n        \n        corr_matrix = pdf.corr()\n        return corr_matrix\n    except Exception as e:\n        print(f\"Error in correlation analysis: {e}\")\n        print(\"For large datasets, compute correlations directly in Snowflake\")\n        return None\n\n# Main function to run the EDA\ndef run_simplified_fraud_eda(df):\n    print(\"=== Simplified Fraud Detection EDA ===\")\n    \n    #Get basic data overview\n    row_count = basic_data_overview(df)\n    \n    # Analyze missing values\n    missing_df = missing_values_analysis(df, row_count)\n    print(\"\\nMissing Values:\")\n    print(missing_df)\n\n   # Analyze correlations\n    corr_matrix = correlation_analysis(df)\n    if corr_matrix is not None:\n        print(\"\\nCorrelation Matrix:\")\n        print(corr_matrix)\n    \n    # Generate summary\n    print(\"\\nEDA Summary Report:\")\n    print(f\"- Dataset has {row_count} transactions\")\n    \n    # Calculate fraud rate (use uppercase IS_FRAUD)\n    fraud_count = df.filter(col(\"IS_FRAUD\") == True).count()\n    fraud_rate = (fraud_count / row_count) * 100 if row_count > 0 else 0\n    print(f\"- Overall fraud rate: {fraud_rate:.2f}%\")\n    \n    # Return all analysis results\n    return {\n        \"row_count\": row_count,\n        \"missing_values\": missing_df,\n        \"correlation\": corr_matrix,\n        \"fraud_rate\": fraud_rate\n    }\n\n# Save key insights to a table\ndef save_eda_summary(session, df, eda_results):\n    # Calculate basic fraud statistics\n    fraud_count = df.filter(col(\"IS_FRAUD\") == True).count()\n    \n    # Create a summary table\n    session.sql(\"\"\"\n    CREATE OR REPLACE TABLE fraud_eda_summary AS\n    SELECT\n        CURRENT_TIMESTAMP() AS analysis_time,\n        {} AS total_transactions,\n        {} AS fraud_transactions,\n        {:.2f} AS fraud_rate_percent\n    \"\"\".format(eda_results[\"row_count\"], fraud_count, eda_results[\"fraud_rate\"])).collect()\n    \n    print(\"Basic EDA summary saved to table: fraud_eda_summary\")\n\n# Example usage:\n# df = session.table(\"fraud_analysis\")\n# eda_results = run_simplified_fraud_eda(df)\n# save_eda_summary(session, df, eda_results)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffcd2433-a352-47fb-879e-e8a9cfcf4535",
   "metadata": {
    "name": "EDA",
    "collapsed": false
   },
   "source": "### Fraud Detection EDA Results Summary\n\n#### Dataset Overview\n- 13,620 transactions with 11 columns\n- Key columns include transaction details, customer information, and complaint data\n\n#### Missing Data Patterns\n- Complaint-related fields (COMPLAINT_TEXT, COMPLAINT_TIME, KEYWORDS) missing for 26.37% of transactions\n- All transaction and customer core data fields are complete (0% missing)\n\n#### Fraud Statistics\n- Overall fraud rate: 10.16% of transactions\n\n#### Correlation Analysis\n- Very weak negative correlation (-0.011) between transaction amount and fraud\n- This suggests fraud occurs across various transaction sizes rather than being concentrated in larger or smaller transactions\n\n#### Data Completeness\n- Transaction core data is complete\n- Missing complaint data likely represents transactions without customer complaints"
  },
  {
   "cell_type": "code",
   "id": "6c18d3b6-cac4-4915-80c4-d81cf4298d99",
   "metadata": {
    "language": "python",
    "name": "eda_result"
   },
   "outputs": [],
   "source": "eda_results = run_simplified_fraud_eda(df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bc5a4ad-a867-4f58-b003-e3f38ad2fdc8",
   "metadata": {
    "name": "Adding_column_FE",
    "collapsed": false
   },
   "source": "### Feature Engineering Summary\n\n#### New Features Created\n\n* **TRANSACTION_TIME**\n  * Transforms the transaction timestamp string into a proper timestamp data type\n  * Enables date/time operations and analysis\n  * Essential for time-based fraud pattern detection\n\n* **SENTIMENT_SCORE**\n  * Leverages Snowflake's built-in Cortex NLP capabilities\n  * Analyzes the sentiment of customer complaint text\n  * Negative sentiment may correlate with fraudulent transactions\n  * Provides a numeric score that can be used in fraud detection models\n\n* **TRANSACTION_DAY**\n  * Extracts the day name (Monday, Tuesday, etc.) from transaction timestamp\n  * Allows for analysis of fraud patterns by day of week\n  * Helps identify if certain days have higher fraud rates\n\n### Implementation Method\n* Uses Snowflake Snowpark API's dictionary-based feature transformation\n* Applies all transformations in a single dataframe operation\n* More efficient than sequential column additions"
  },
  {
   "cell_type": "code",
   "id": "80c4a42d-0e07-4f86-83af-2f0c43d9eaf8",
   "metadata": {
    "language": "python",
    "name": "define_features",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Create a dict with keys for feature names and values containing transform code\nfrom snowflake.snowpark.functions import call_udf\nfeature_eng_dict = dict()\nfeature_eng_dict[\"TRANSACTION_TIME\"] = to_timestamp(\"TRANSACTION_TIME\")\nfeature_eng_dict[\"SENTIMENT_SCORE\"] = call_udf(\"SNOWFLAKE.CORTEX.SENTIMENT\", col(\"complaint_text\"))\nfeature_eng_dict[\"TRANSACTION_DAY\"] = dayname(col(\"transaction_time\"))\ndf = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "132c4cb1-b816-4867-85f8-fe1ecdea7f03",
   "metadata": {
    "language": "python",
    "name": "df_show_v"
   },
   "outputs": [],
   "source": "df.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0d005a7-b1d6-4910-b9fa-aa22e3d2410e",
   "metadata": {
    "name": "Feature_Store",
    "collapsed": false
   },
   "source": "### Feature Engineering & Store Initialization Summary\n\n#### Feature Store Setup\n* **Initialization Mode**  \n  - Use `CREATE_IF_NOT_EXIST` for first-time setup:  \n    ```\n    from snowflake.ml.feature_store import FeatureStore\n    fs = FeatureStore(mode=FeatureStore.Mode.CREATE_IF_NOT_EXIST)\n    ```\n  - Subsequent connections use `FAIL_IF_NOT_EXIST` mode\n  - Requires pre-existing database (Feature Store won't create it)\n\n* **Core Components**  \n  - **Entities:** Define core business objects (e.g., `Customer`, `Transaction`)  \n  - **Feature Definitions:** Create reusable feature transformations  \n  - **Metadata:** Track feature lineage and versioning\n\n#### Behavioral Metrics Generation\n* **Data Sources**  \n  - Primary table: `CREDITCARD_TRANSACTIONS`\n  - Supplemental tables: User activity logs\n\n* **Key Transformations**  \n"
  },
  {
   "cell_type": "code",
   "id": "2dd50344-d2fb-4ac8-adf0-78a6c3201ba2",
   "metadata": {
    "language": "python",
    "name": "define_feature_store"
   },
   "outputs": [],
   "source": "fs = FeatureStore(\n    session=session, \n    database=session.get_current_database(), \n    name=session.get_current_schema(), \n    default_warehouse=session.get_current_warehouse(),\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62c231f3-cb38-42d1-82cd-01c3c7650cf9",
   "metadata": {
    "language": "python",
    "name": "see_timespan"
   },
   "outputs": [],
   "source": "df.select(min('TRANSACTION_TIME'), max('TRANSACTION_TIME'))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fa62e2d-b2a2-4f7f-8e3b-30ef4f3b608f",
   "metadata": {
    "language": "python",
    "name": "df_explain",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df.explain()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37e0bba9-1133-4e9e-8a6e-aa6dc2394c05",
   "metadata": {
    "name": "customertrans_entity",
    "collapsed": false
   },
   "source": "### Feature Store Entity Registration (Concise)\n\n*   **Entities Defined:** `CUSTOMER` \n*   **Purpose:** Represent key data objects for feature lookup.\n*   **`CUSTOMER` Entity:**\n    *   `primary_keys`: `CUSTOMER_ID` (string)\n    *   Links customer data across tables.\n*   **Implementation:**\n    *   Register entities with `fs.register_entity()`.\n*   **Impact:** Enables consistent feature access & lineage tracking.\n"
  },
  {
   "cell_type": "code",
   "id": "d21b0b42-920f-4295-ac55-e1f2a928e734",
   "metadata": {
    "language": "python",
    "name": "load_or_register_entities"
   },
   "outputs": [],
   "source": "# First try to retrieve an existing entity definition, if not define a new one and register\ntry:\n    # Retrieve existing entity\n    customer_entity = fs.get_entity('CUSTOMER_ID_ENTITY') \n    print('Retrieved existing entity')\nexcept:\n    # Define new entity\n    customer_entity = Entity(\n        name = \"CUSTOMER_ID_ENTITY\",\n        join_keys = [\"CUSTOMER_ID\"],\n        desc = \"Features defined on a per customer level\")\n    \n    # Register\n    fs.register_entity(customer_entity)\n    print(\"Registered new entity\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a277df89-db77-4b9a-a924-7b1221ac475c",
   "metadata": {
    "language": "python",
    "name": "list_entities"
   },
   "outputs": [],
   "source": "fs.list_entities()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1be891f-3225-46de-b688-b1f1c31981a4",
   "metadata": {
    "language": "python",
    "name": "create_feature_df"
   },
   "outputs": [],
   "source": "#Create a dataframe with just the ID, timestamp, and engineered features. We will use this to define our feature view\nfeature_df = df.select([\"CUSTOMER_ID\"]+list(feature_eng_dict.keys()))\nfeature_df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e19f2b0d-4fc2-41a4-959f-a0d549e0d413",
   "metadata": {
    "name": "Feature_Views",
    "collapsed": false
   },
   "source": "### Feature View Summary: FRAUD_FEATURES\n\n*   **Purpose**: Group logically-related customer features for streamlined access.\n\n*   **Key Components**:\n\n    *   `name`: \"FRAUD\\_FEATURES\"\n    *   `entities`: \\[`customer_entity`] (links to customer data)\n    *   `feature_df`:  Snowpark DataFrame containing feature generation logic.\n    *   `timestamp_col`: \"TRANSACTION\\_TIME\" (for time-series data)\n\n*   **Workflow**:\n\n    1.  Define a Snowpark DataFrame (`feature_df`) with the desired feature transformations.\n    2.  Create a `FeatureView` instance.\n    3.  Register the `FeatureView` using `fs.register_feature_view()`.\n\n*   **Configuration**:\n\n    *   `version`: `\"1\"` (initial version)\n    *   `overwrite`: `True` (allows overwriting existing versions)\n\n*   **Impact**:\n\n    *   Provides a consistent and versioned interface for retrieving customer fraud features.\n    *   Simplifies feature retrieval for machine learning models.\n\n* **Requirement**\n\n    * DataFrame should contain join keys column for entity and timestamp column for time-based data.\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "b7d79ea2-1b91-4c60-98a8-f54940097562",
   "metadata": {
    "language": "python",
    "name": "feature_veiw_creation"
   },
   "outputs": [],
   "source": "#define and register feature view\nfraud_fv = FeatureView(\n    name=\"FRAUD_FEATURES\",\n    entities=[customer_entity],\n    feature_df=feature_df,\n    timestamp_col=\"TRANSACTION_TIME\")\n\nfraud_fv = fs.register_feature_view(fraud_fv, version=\"1\", overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85d0d177-3dd4-4b88-9d80-0d5618caf9e1",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "451304eb-77ee-4e5c-a9f9-d05b25da33aa",
   "metadata": {
    "language": "python",
    "name": "feature_view"
   },
   "outputs": [],
   "source": "fraud_fv",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b7067c5-5048-4a55-a462-31da4d337a10",
   "metadata": {
    "language": "python",
    "name": "show_feature_views"
   },
   "outputs": [],
   "source": "fs.list_feature_views()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90323264-df3d-4679-ab8b-1c9ea3961120",
   "metadata": {
    "name": "Writeup_featurestore",
    "collapsed": false
   },
   "source": "### Training Dataset Generation Summary\n\n*   **Workflow Completion:** Marks the completion of database object setup and Feature Store Producer workflow.\n\n*   **Data Availability:** Indicates that generated data and features are ready for consumption (with appropriate privileges).\n\n*   **Dataset Generation:** Highlights the transition to generating a training dataset.\n\n*   **Spine DataFrame:**\n    *   Defines a \"spine\" DataFrame.\n    *   Serves as a request template for the dataset.\n    *   Specifies entities, labels, and timestamps.\n\n*   **FeatureStore.generate\\_dataset():**\n    *   Employs `FeatureStore.generate_dataset()` to create the training set.\n    *   Utilizes Feature Views.\n\n*   **AS-OF Join:**\n    *   Feature Store attaches feature values along the spine using an AS-OF join.\n    *   Efficiently combines and serves relevant, point-in-time correct feature data.\n"
  },
  {
   "cell_type": "code",
   "id": "8c927fae-33fa-4760-a383-310e5119468c",
   "metadata": {
    "language": "python",
    "name": "generate_dataset"
   },
   "outputs": [],
   "source": "ds = fs.generate_dataset(\n    name=\"FRAUD_DETECTION_DATASET_V1\",\n    spine_df=df.drop(\"SENTIMENT_SCORE\", \"complaint_text\",\n \"TRANSACTION_DAY\",\"KEYWORDS\",\"COMPLAINT_TIME\"),\n    features=[fraud_fv],\n    spine_timestamp_col=\"TRANSACTION_TIME\",\n    spine_label_cols=[\"IS_FRAUD\"]\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f6a19d4-86ac-4118-b451-1014b0f5cf5c",
   "metadata": {
    "language": "python",
    "name": "convert_dataset_to_snowpark_and_pandas"
   },
   "outputs": [],
   "source": "ds_sp = ds.read.to_snowpark_dataframe()\nds_sp.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3eae0ad9-e2d3-4974-9e92-985f1d683894",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f8b36b5-eb27-444a-8ccb-3de43f268e18",
   "metadata": {
    "name": "Feature_engineering_one_hot_encoding",
    "collapsed": false
   },
   "source": "#### Feature Engineering: One-Hot Encoding Summary\n\n*   **Objective**: Convert categorical string columns into numerical representations suitable for machine learning.\n\n*   **Implementation**: Utilizes Snowflake ML's `OneHotEncoder`.\n\n*   **Steps:**\n\n    1.  **Identify Categorical Columns:**\n        *   Select columns with `StringType` (excluding `CUSTOMER_ID` and `TRANSACTION_ID`).\n        *   `OHE_COLS` list stores names of categorical columns.\n\n    2.  **Create Output Column Names:**\n        *   `OHE_POST_COLS` list generates new column names by appending \"\\_OHE\" to original names.\n\n    3.  **Instantiate OneHotEncoder:**\n        *   `snowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols=OHE_POST_COLS, drop_input_cols=True)`\n        *   `input_cols`: Specifies columns to encode.\n        *   `output_cols`: Specifies names for encoded columns.\n        *   `drop_input_cols=True`: Removes original categorical columns.\n\n    4.  **Fit and Transform:**\n        *   `ds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)`\n        *   `fit()`: Learns the unique categories from the input data.\n        *   `transform()`: Applies the encoding to create new numerical columns.\n\n    5.  **Result:**\n        *   `ds_sp_ohe`: New DataFrame with one-hot encoded columns.\n        *   Original string columns are replaced by numerical representations.\n\n*   **Benefits:**\n\n    *   Enables machine learning algorithms to process categorical data effectively.\n    *   Avoids imposing ordinal relationships on categorical features.\n"
  },
  {
   "cell_type": "code",
   "id": "c0b22898-ddf1-41dc-ad03-b5a3ceaf9598",
   "metadata": {
    "language": "python",
    "name": "one_hot_encoding"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.types import StringType\nimport snowflake.ml.modeling.preprocessing as snowml\n\n# Select categorical columns (columns with StringType), excluding CUSTOMER_ID and TRANSACTION_ID\nOHE_COLS = [col.name for col in ds_sp.schema.fields\n            if isinstance(col.datatype, StringType)\n            and col.name not in ('CUSTOMER_ID', 'TRANSACTION_ID')]\n\n# Create output column names for one-hot encoding\nOHE_POST_COLS = [i + \"_OHE\" for i in OHE_COLS]\n\n# Encode categorical columns to numeric columns using OneHotEncoder\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols=OHE_POST_COLS, drop_input_cols=True)\n\n# Fit and transform the dataset\nds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n\n# Print the resulting column names\nds_sp_ohe.columns\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a60c1df-fda5-4990-a3d8-edd6b3e91f18",
   "metadata": {
    "name": "train_snowmlmodel",
    "collapsed": false
   },
   "source": "### training the model trying snowml model \n"
  },
  {
   "cell_type": "code",
   "id": "625874f0-e10e-49be-b83d-65c1db894981",
   "metadata": {
    "language": "python",
    "name": "simulate_new_dataset"
   },
   "outputs": [],
   "source": "ds_sp_ohe.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96297cac-0a0e-4bdc-8be6-162d8016ad0c",
   "metadata": {
    "language": "python",
    "name": "train_test_split"
   },
   "outputs": [],
   "source": "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=216)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cd3338d-2cdf-4833-a699-17f6de8b1b96",
   "metadata": {
    "language": "python",
    "name": "fill_na"
   },
   "outputs": [],
   "source": "train = train.fillna(0)\ntest = test.fillna(0)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3f10cb5-867c-4dd0-a01e-d9ef12e80efb",
   "metadata": {
    "language": "python",
    "name": "train_show"
   },
   "outputs": [],
   "source": "train.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2ce9ce4-89c9-4177-8c1e-d41c9895289e",
   "metadata": {
    "language": "python",
    "name": "test_show"
   },
   "outputs": [],
   "source": "test.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b232775e-7e8b-4cc9-8002-d20b35e357af",
   "metadata": {
    "language": "python",
    "name": "define_themodel",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\n\nsnow_xgb_tree = XGBClassifier(\n    input_cols=train.drop([\"IS_FRAUD\", \"TRANSACTION_TIME\", \"CUSTOMER_ID\",'TRANSACTION_ID']).columns,\n    label_cols=train.select(\"IS_FRAUD\").columns,\n    output_cols=\"FRAUD_PREDICTION\",\n    learning_rate = 0.75,\n    ##tree_method=\"exact\",\n    ##n_estimators=5,\n    booster = 'gbtree'\n)\n\nsnow_xgb_linear = XGBClassifier(\n    input_cols=train.drop([\"IS_FRAUD\", \"TRANSACTION_TIME\", \"CUSTOMER_ID\",\"TRANSACTION_ID\"]).columns,\n    label_cols=train.select(\"IS_FRAUD\").columns,\n    output_cols=\"FRAUD_PREDICTION\",\n    # tree_method=\"exact\",\n    # n_estimators=10,\n    booster = 'gblinear'\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ce24c71-dfdf-4f9c-b43d-217e0216504a",
   "metadata": {
    "language": "python",
    "name": "train_tree",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nsnow_xgb_tree.fit(train)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "465bab39-0b7d-4d9d-a485-9be40ecf1c34",
   "metadata": {
    "language": "python",
    "name": "train_linear",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snow_xgb_linear.fit(train)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd2afb56-86d6-46c4-aec1-09555869ba46",
   "metadata": {
    "language": "python",
    "name": "compute_predictions_and_perf_metrics"
   },
   "outputs": [],
   "source": "from sklearn.metrics import f1_score, precision_score, recall_score\ntest_preds_tree = snow_xgb_tree.predict(test).select([\"IS_FRAUD\", \"FRAUD_PREDICTION\"]).to_pandas()\ntest_preds_linear = snow_xgb_linear.predict(test).select([\"IS_FRAUD\", \"FRAUD_PREDICTION\"]).to_pandas()\n\nf1_tree = f1_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\nf1_linear = f1_score(test_preds_linear.IS_FRAUD, test_preds_linear.FRAUD_PREDICTION)\n\n\nprecision_tree = precision_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\nprecision_linear = precision_score(test_preds_linear.IS_FRAUD, test_preds_linear.FRAUD_PREDICTION)\n\nrecall_tree = recall_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\nrecall_linear = recall_score(test_preds_linear.IS_FRAUD, test_preds_linear.FRAUD_PREDICTION)\n\n\nprint(f'GB Tree: \\n f1: {f1_tree} \\n precision {precision_tree} \\n recall: {recall_tree}')\nprint(f'GB Linear: \\n f1: {f1_linear} \\n precision {precision_linear} \\n recall: {recall_linear}')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15fbb748-413b-4e8a-bdcc-ed4fedb09c17",
   "metadata": {
    "language": "python",
    "name": "sklearn_implementation"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c15e5c83-b0e9-40a8-93dc-540468d54d99",
   "metadata": {
    "language": "python",
    "name": "sklearn_train_test"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nds_sp_pandas = ds_sp.to_pandas()\n\n# Assuming 'ds_sp' is a valid pandas DataFrame\nX = ds_sp_pandas[['DEVICE_TYPE', 'MERCHANT_CATEGORY', 'TRANSACTION_DAY', \n                  'TRANSACTION_AMOUNT', 'SENTIMENT_SCORE']]\ny = ds_sp_pandas['IS_FRAUD']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Optional: Display shapes to verify the split\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "720010c4-fd6c-4e0d-b61f-01c3b0e40920",
   "metadata": {
    "language": "python",
    "name": "skllearn_pipeline_modelling"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\n\n# Define categorical and numerical columns\ncategorical_features = ['DEVICE_TYPE', 'MERCHANT_CATEGORY', 'TRANSACTION_DAY']\nnumerical_features = ['TRANSACTION_AMOUNT', 'SENTIMENT_SCORE']\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline([\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ]), numerical_features),\n        ('cat', Pipeline([\n            ('imputer', SimpleImputer(strategy='most_frequent')),\n            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n        ]), categorical_features)\n    ]\n)\n\n\n# Define models\nmodels = {\n    \"XGBoost\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n    ]),\n    \"RandomForest\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier())\n    ]),\n    \"LogisticRegression\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', LogisticRegression())\n    ])\n}\n\nbest_model = None\nbest_score = 0\n\nfor name, model in models.items():\n    print(f\"Training {name} model...\")\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test, preds)\n    print(f\"{name} Accuracy: {score:.4f}\")\n\n    if score > best_score:\n        best_model = model\n        best_score = score\n\nprint(f\"Best model: {type(best_model.named_steps['classifier']).__name__} with accuracy {best_score:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dd079cea-1aa3-412f-b3a2-8e3712efba07",
   "metadata": {
    "name": "model_registry_md_snowml",
    "collapsed": false
   },
   "source": "#### Model Registry, Deployment, and Serving Summary (with Snowpark Container Services)\n\n*   **Model Registry**:\n    *   **Purpose**: Stores and manages ML models within Snowflake.\n    *   **Functionality**:\n        *   Versioning\n        *   Metadata Tracking\n        *   Model Performance Metrics\n    *   **Workflow**:\n        1.  Create a registry connected to Snowflake database and schema.\n        2.  Log the model to the registry, including model files and any relevant metadata, along with model versions.\n        3.  Set performance metrics to track model performance.\n\n*   **Model Deployment (Multiple Options):**\n    *   **Purpose**: Make the model available for inference or prediction tasks within Snowflake.\n    *   **Deployment Strategies**:\n        *   **User-Defined Function (UDF) Deployment**: Package the model as a UDF in Snowflake, allowing direct SQL calls for inference.\n        *   **Snowpark Integration**: Use Snowpark to load the model and perform predictions on data within a Snowpark session.\n        *   **Snowpark Container Services (SPCS)**: Package the model and inference code into a container, then deploy it as a service in SPCS. This allows for greater flexibility in the inference environment and can support complex models.\n\n*   **Model Serving/Inference**:\n    *   **Purpose**: Use the deployed model to generate predictions on new data.\n    *   **Methods**:\n        *   **SQL Inference**: Call the deployed model using SQL queries (if deployed as UDF).\n        *   **Snowpark Inference**: Load and use the model within a Snowpark session to make predictions.\n        *   **SPCS Inference**: Send requests to the deployed SPCS service to get predictions. This can be done from external applications or from within Snowflake.\n    *   **Framework**: Cortex, Snowpark, SPCS\n"
  },
  {
   "cell_type": "code",
   "id": "1b682614-1012-4549-9e14-6ba5ddf65e7f",
   "metadata": {
    "language": "python",
    "name": "define_model_registry"
   },
   "outputs": [],
   "source": "from snowflake.ml._internal.utils import identifier\n#Create a snowflake model registry object \ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\nmodel_registry = Registry(session=session, \n                    database_name=session.get_current_database(), \n                    schema_name=session.get_current_schema(),\n                    options={\"enable_monitoring\": True})",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04e7dec9-6f5e-49c9-a72b-00c0280eb4d0",
   "metadata": {
    "language": "python",
    "name": "register_model_version_xgb"
   },
   "outputs": [],
   "source": "#Deploy the tree booster model to the model registry\n# Define model name\nmodel_name = \"FRAUD_ANALYSIS_XGB\"\ntree_version_name = 'V2'\n\ntry:\n    mv_tree = model_registry.get_model(model_name).version(tree_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    mv_tree = model_registry.log_model(\n        model_name=model_name,\n        model=snow_xgb_tree, \n        version_name=tree_version_name,\n        comment = \"snow ml model built off feature store using tree booster\",\n    )\n    mv_tree.set_metric(metric_name=\"F1_score\", value=f1_tree)\n    mv_tree.set_metric(metric_name=\"Precision_score\", value=precision_tree)\n    mv_tree.set_metric(metric_name=\"Recall_score\", value=recall_tree)\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "648f3e08-de55-4ccc-854a-761f8205ec82",
   "metadata": {
    "language": "python",
    "name": "register_model_version_linear"
   },
   "outputs": [],
   "source": "#Deploy the tree booster model to the model registry\n# Define model name\nmodel_name = \"FRAUD_ANALYSIS_LINEAR\"\ntree_version_name = 'V1'\n\ntry:\n    mv_tree = model_registry.get_model(model_name).version(tree_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    mv_linear = model_registry.log_model(\n        model_name=model_name,\n        model=snow_xgb_linear, \n        version_name=tree_version_name,\n        comment = \"snow ml model built off feature store using linear booster\",\n    )\n    mv_linear.set_metric(metric_name=\"F1_score\", value=f1_linear)\n    mv_linear.set_metric(metric_name=\"Precision_score\", value=precision_linear)\n    mv_linear.set_metric(metric_name=\"Recall_score\", value=recall_linear)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0467afc0-7926-4958-a748-58a3cfa86c2a",
   "metadata": {
    "language": "python",
    "name": "register_model_version_sklearn",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "#Deploy the sklearn model to the model registry\n# Define model name\nmodel_name = \"FRAUD_ANALYSIS\"\nversion_name = 'V1'\n\ntry:\n    mv = model_registry.get_model(model_name).version(version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    mv  = model_registry.log_model(\n        model_name=model_name,\n        model=best_model, \n        version_name=version_name,\n        sample_input_data=X_train,\n        comment = \"sklearn model build\",\n    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4631a4bb-3fc1-4ca8-b0b0-a16192e4089a",
   "metadata": {
    "language": "python",
    "name": "model_registry_show"
   },
   "outputs": [],
   "source": "model_registry.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "385d1311-cebe-446f-acfd-44e894e96afd",
   "metadata": {
    "language": "python",
    "name": "model_registry_delete",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "## model_registry.delete_model(\"FRAUD_ANALYSIS\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41dc2320-6dab-402e-810d-1ac09ea60d62",
   "metadata": {
    "language": "python",
    "name": "show_model_versions"
   },
   "outputs": [],
   "source": "model_registry.get_model(\"FRAUD_ANALYSIS\").show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "835d0aba-fa5d-48e3-a6df-7645cac1032d",
   "metadata": {
    "language": "python",
    "name": "model_retrieving_metrics"
   },
   "outputs": [],
   "source": "### retrieving sklearn - frad model - version 1 \nreg_model = model_registry.get_model(\"FRAUD_ANALYSIS\").version(\"v1\")\n### retrieving sklearn - frad model - version 1 \nreg_model_tree = model_registry.get_model(\"FRAUD_ANALYSIS_XGB\").version(\"v2\")\n\nreg_model_linear = model_registry.get_model(\"FRAUD_ANALYSIS_LINEAR\").version(\"v1\")\n\n\nprint(mv)\nprint(mv.show_metrics())\n\nprint(mv_tree)\nprint(mv_tree.show_metrics())\n\nprint(mv_linear)\nprint(mv_linear.show_metrics())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d21d856-83ac-4a48-8770-009c1677b769",
   "metadata": {
    "name": "intro_batch_inferencing",
    "collapsed": false
   },
   "source": "# Batch Inferencing on warehouse"
  },
  {
   "cell_type": "markdown",
   "id": "45871620-2429-451a-87f4-f6a2168f4ab1",
   "metadata": {
    "name": "batch_inferencing_tree_model",
    "collapsed": false
   },
   "source": "#### Two model what we have trained Sklearn & snowml lets predict using both the models "
  },
  {
   "cell_type": "code",
   "id": "54de8e92-d01d-4d65-88ed-bad264e36856",
   "metadata": {
    "language": "python",
    "name": "predict_from_snowml_model"
   },
   "outputs": [],
   "source": "reg_preds_tree = mv_tree.run(test, function_name = \"predict\")\nreg_probs_tree_prob = mv_tree.run(test, function_name=\"predict_proba\") \nreg_preds_tree.show(2)\nreg_probs_tree_prob.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3972da8d-3686-476f-83c3-41118c9a1de3",
   "metadata": {
    "language": "python",
    "name": "prediction_on_complete_dataset"
   },
   "outputs": [],
   "source": "### Extracting customer id, transaction Id, and prediction \n\nreg_probs_tree_prob_complete = mv_tree.run(ds_sp_ohe, \n                            function_name=\"predict_proba\").select(\"CUSTOMER_ID\", \n                        \"TRANSACTION_ID\", \"PREDICT_PROBA_0\", \"PREDICT_PROBA_1\" ) \n\nreg_probs_tree_prob_complete.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6d6b520-212a-4183-b4e0-62c02fdcc08a",
   "metadata": {
    "language": "python",
    "name": "Master_dataset"
   },
   "outputs": [],
   "source": "### Earlier data set before split \nds_sp.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a66ad24e-41f3-4832-ab74-7ff9cf3d3a04",
   "metadata": {
    "language": "python",
    "name": "master_data_creation_python"
   },
   "outputs": [],
   "source": "joined_df = ds_sp.join(\n    reg_probs_tree_prob_complete,\n    on=[\"CUSTOMER_ID\", \"TRANSACTION_ID\"],\n    join_type=\"left\"\n)\n\njoined_df.show(2)\n# # Save the dataframe\n#joined_df.write.mode(\"overwrite\").save_as_table(\"FT_PREDICTION_FINAL\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98270841-d5c9-4104-9673-b95d0c8c3a5f",
   "metadata": {
    "language": "sql",
    "name": "final_dataset"
   },
   "outputs": [],
   "source": "select * from FT_PREDICTION_FINAL limit 2; ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c8ae24c-bcaf-4c9e-982b-4fca8ab3b923",
   "metadata": {
    "name": "Prediction_using_sklearn",
    "collapsed": false
   },
   "source": "#### This was an additional code to predict using sciktlearn model "
  },
  {
   "cell_type": "code",
   "id": "b93efb51-814f-4181-9c4e-0fa892828bfe",
   "metadata": {
    "language": "python",
    "name": "predict_prob_from_sklearn_model"
   },
   "outputs": [],
   "source": "reg_probs_sk = mv.run(X_test, function_name=\"predict_proba\") \nreg_preds_sk = mv.run(X_test, function_name = \"predict\")\nprint(reg_probs_sk.head(2))\nprint(reg_preds_sk.head(2))\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc22a252-fe1d-4992-a3e1-9670dc87f375",
   "metadata": {
    "name": "feature_Importance",
    "collapsed": false
   },
   "source": "### Feature importance and score "
  },
  {
   "cell_type": "code",
   "id": "98ccdaa7-529a-4077-821f-b0b29055b8ce",
   "metadata": {
    "language": "python",
    "name": "feature_importance"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Get feature names from preprocessor\nfeature_names = (models[\"RandomForest\"]\n                 .named_steps[\"preprocessor\"]\n                 .transformers_[0][2] +  # Numerical features\n                 list(models[\"RandomForest\"].named_steps[\"preprocessor\"]\n                      .transformers_[1][1]\n                      .named_steps[\"encoder\"]\n                      .get_feature_names_out(categorical_features)))  # Encoded categorical features\n\n# Get feature importance from the RandomForestClassifier\nimportances = best_model.named_steps[\"classifier\"].feature_importances_\n\n# Convert to DataFrame for better readability\nfeature_importance_df = pd.DataFrame(\n    {\"FEATURE\": feature_names, \"IMPORTANCE\": importances}\n).sort_values(by=\"IMPORTANCE\", ascending=False)\n\n# Print feature importances\n#print(feature_importance_df)\n\nsnowpark_df = session.create_dataframe(feature_importance_df)\n\n#snowpark_df\n#snowpark_df.write.mode(\"overwrite\").save_as_table(\"feature_importance_df\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "403bc0b8-087a-48ec-af33-cffe199b5320",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "select * from feature_importance_df;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c1403f3-069d-4976-9e80-2ab386136a72",
   "metadata": {
    "language": "python",
    "name": "feature_importance_graph"
   },
   "outputs": [],
   "source": "# Plot feature importance\nplt.figure(figsize=(10, 5))\nplt.barh(feature_importance_df[\"FEATURE\"], feature_importance_df[\"IMPORTANCE\"])\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"FEATURE\")\nplt.title(\"Feature Importance for RandomForestClassifier Model\")\nplt.gca().invert_yaxis()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a4d6cc5-46b7-4511-aa0d-2d69e31df8fa",
   "metadata": {
    "name": "Inferen_SPCS",
    "collapsed": false
   },
   "source": "#### Real time inferencing deployment on Snowpark Container Services\n#### Batch predicting using SPCS "
  },
  {
   "cell_type": "code",
   "id": "96b16265-ab9a-400f-a1c8-ed6f013367e8",
   "metadata": {
    "language": "python",
    "name": "configuring_spcs"
   },
   "outputs": [],
   "source": "# Define model name\nmodel_name = \"FRAUD_ANALYSIS_SPCS\"\nimage_repo_name = \"AI_ML_REPO\"\ncp_name = \"AI_ML_CP\"\nnum_spcs_nodes = '3'\nspcs_instance_family = 'CPU_X64_L'\nservice_name = 'FRAUD_DETECTION_SERVICE'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d130b561-a6e6-4e6c-a07c-6adcf5234ea3",
   "metadata": {
    "language": "python",
    "name": "session_parameters"
   },
   "outputs": [],
   "source": "current_database = session.get_current_database().replace('\"', '')\ncurrent_schema = session.get_current_schema().replace('\"', '')\nextended_image_repo_name = f\"{current_database}.{current_schema}.{image_repo_name}\"\nextended_service_name = f'{current_database}.{current_schema}.{service_name}'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4288ae01-2638-4b6d-8fe5-e69b1a5c0ee5",
   "metadata": {
    "language": "python",
    "name": "starting_session"
   },
   "outputs": [],
   "source": "session.sql(f\"alter compute pool if exists {cp_name} stop all\").collect()\nsession.sql(f\"drop compute pool if exists {cp_name}\").collect()\nsession.sql(f\"create compute pool {cp_name} min_nodes={num_spcs_nodes} max_nodes={num_spcs_nodes} instance_family={spcs_instance_family} auto_resume=True auto_suspend_secs=300\").collect()\nsession.sql(f\"describe compute pool {cp_name}\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2140b8f6-5445-48d2-834d-69cb66730a24",
   "metadata": {
    "language": "python",
    "name": "Repo_created"
   },
   "outputs": [],
   "source": "session.sql(f\"create image repository if not exists {extended_image_repo_name}\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56eafa3d-3a09-4435-96c0-2d0e30323158",
   "metadata": {
    "language": "python",
    "name": "create_service_prediction",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "mv_tree.create_service(\n    service_name=extended_service_name,\n    service_compute_pool=cp_name,\n    image_repo=extended_image_repo_name,\n    ingress_enabled=True,\n    max_instances=int(num_spcs_nodes),\n    build_external_access_integration=\"ALLOW_ALL_INTEGRATION\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d30c03fd-25a2-4dd6-9102-ff3009ecc031",
   "metadata": {
    "language": "python",
    "name": "List_services"
   },
   "outputs": [],
   "source": "mv_tree.list_services()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "640e472e-8ddf-45e5-aec7-4744006b04b0",
   "metadata": {
    "language": "python",
    "name": "Checing_service"
   },
   "outputs": [],
   "source": "session.sql(f\"SELECT VALUE:status::VARCHAR as SERVICESTATUS, VALUE:message::VARCHAR as SERVICEMESSAGE FROM TABLE(FLATTEN(input => parse_json(system$get_service_status('{service_name}')), outer => true)) f\").show(100)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9dce4c0d-2534-4ccb-a783-7c0f16e69fdf",
   "metadata": {
    "language": "python",
    "name": "prediction_url"
   },
   "outputs": [],
   "source": "session.sql(f\"show endpoints in service {service_name}\").collect()[0][\"ingress_url\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e68199c0-1d90-4aeb-b0a1-be2fa4f84c90",
   "metadata": {
    "language": "python",
    "name": "test_dataset"
   },
   "outputs": [],
   "source": "test.limit(1).show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "094602d7-2e7e-4e99-b92a-66dd3bf69015",
   "metadata": {
    "name": "Batch_inference",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "9a3f1814-3e2c-4a6f-9334-2dd8be4c6421",
   "metadata": {
    "language": "python",
    "name": "batch_predict_spcs"
   },
   "outputs": [],
   "source": "mv_tree.run(test.limit(4) , service_name=service_name, function_name=\"predict\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32d4b9df-f59b-452e-a13c-f0f583c0d193",
   "metadata": {
    "language": "python",
    "name": "service_predict_prob"
   },
   "outputs": [],
   "source": "mv_tree.run(test.limit(4) , service_name=service_name, function_name=\"predict_proba\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e87e3371-a827-4f48-a125-edc883573b6d",
   "metadata": {
    "name": "model_explanability",
    "collapsed": false
   },
   "source": "# Model explainability"
  },
  {
   "cell_type": "code",
   "id": "4d66755b-5242-439c-85d9-b7169d45c5e8",
   "metadata": {
    "language": "python",
    "name": "explanability"
   },
   "outputs": [],
   "source": "shap_vals = mv_tree.run(test.sample(n=1000), function_name=\"explain\")\nshap_vals.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74fa9ff9-e267-42e9-8bf5-c85c55cf08c6",
   "metadata": {
    "language": "python",
    "name": "explanability_pandas"
   },
   "outputs": [],
   "source": "## convert in to pandas\nshap_pd = shap_vals.to_pandas()\nshap_pd.head(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2153f740-5205-4229-8344-2c54886b7bde",
   "metadata": {
    "language": "python",
    "name": "pip_install_shap"
   },
   "outputs": [],
   "source": "# !pip install shap",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e9b8730-c316-408a-ba82-6397a50d36de",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# column_list = shap_pd.columns.tolist()\n# print(column_list)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05078f12-e8e5-4423-b700-82f0696900dc",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# column_list = just_input_vals.columns.tolist()\n# print(column_list)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32213419-3f3a-4d89-8145-4e3fdcc45539",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# shap_pd.columns.intersection(just_input_vals.columns)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edaf516a-d057-4d21-a142-3ff724912f09",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "# just_shap = shap_pd.iloc[:, 31:]\n# just_input_vals = shap_pd.iloc[:, :31]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44a0d309-a46f-49fc-aab9-7e7adf1d6fe5",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# just_input_vals = shap_pd.iloc[:, :31].drop([\"CUSTOMER_ID\",\"IS_FRAUD\", \"TRANSACTION_TIME\"], axis=1)\n# just_input_vals.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "777a9482-0b8a-4acf-8486-685f8b8377ab",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# just_shap = shap_pd.iloc[:, 31:]\n# just_shap.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2710a0d7-50fa-4c3d-8720-94dd82c63ff8",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "# import shap \n# just_shap = shap_pd.iloc[:, 30:]\n# just_input_vals = shap_pd.iloc[:, :30].drop([\"CUSTOMER_ID\",\"IS_FRAUD\",\"TRANSACTION_ID\", \"TRANSACTION_TIME\"], axis=1)\n# shap.summary_plot(np.array(just_shap), just_input_vals, feature_names = just_input_vals.columns)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8caeee5-217f-4d3f-8143-48bd74854fe0",
   "metadata": {
    "language": "python",
    "name": "scatterplot"
   },
   "outputs": [],
   "source": "# import seaborn as sns\n\n# sns.scatterplot(data = shap_pd, x =\"LOAN_PURPOSE_NAME_Home purchase\", y = \"LOAN_PURPOSE_NAME_Home purchase_explanation\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3954c74b-db39-4198-810c-4ea078a9c3f5",
   "metadata": {
    "language": "python",
    "name": "seaborn"
   },
   "outputs": [],
   "source": "# import seaborn as sns\n\n# income_0_to_1M = shap_pd[(shap_pd.INCOME>0) & (shap_pd.INCOME<1000000)]\n# sns.scatterplot(data = income_0_to_1M, x =\"INCOME\", y = \"INCOME_explanation\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6a667b7-4c9d-464a-a6b8-c20eacdcb427",
   "metadata": {
    "name": "Model_monitor_setup",
    "collapsed": false
   },
   "source": "## Model Monitoring setup"
  },
  {
   "cell_type": "code",
   "id": "b5c0dde0-13bf-477f-b4cf-40357e3315f8",
   "metadata": {
    "language": "python",
    "name": "TABLE_MONITOR_SETUP"
   },
   "outputs": [],
   "source": "train.write.save_as_table(\"DEMO_FRAUD_TRAIN\", mode=\"overwrite\")\ntest.write.save_as_table(\"DEMO_FRAUD_TEST\", mode=\"overwrite\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b166b9e2-86cf-473a-9566-91faefbcdb3f",
   "metadata": {
    "language": "python",
    "name": "SPROC_PROCEDURE"
   },
   "outputs": [],
   "source": "from snowflake import snowpark\nfrom snowflake.ml.registry import Registry\nimport joblib\nimport os\nimport logging\nfrom snowflake.ml.modeling.pipeline import Pipeline\nimport snowflake.ml.modeling.preprocessing as pp\nfrom snowflake.snowpark.types import StringType, IntegerType\nimport snowflake.snowpark.functions as F\n\n\ndef demo_inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n    \n    database=session.get_current_database()\n    schema=session.get_current_schema()\n    reg = Registry(session=session)\n    m = reg.get_model(modelname)  # Fetch the model using the registry\n    mv = m.version(modelversion)\n    \n    input_table_name=table_name\n    pred_col = f'{modelversion}_PREDICTION'\n\n    # Read the temporary DataFrame\n    df = session.table(input_table_name)\n\n    # Perform prediction using the model\n    results = mv.run(df, function_name=\"predict\")  # 'results' is the output DataFrame with predictions\n    results = results.withColumnRenamed(\"FRAUD_PREDICTION\", pred_col)\n\n    # Write results to a temporary Snowflake table\n    temp_results_table = \"DEMO_TEMP_PREDICTION_RESULTS\"\n    results.write.save_as_table(temp_results_table, mode='overwrite')\n\n    \n    # # Execute the update statement\n\n    df = df.with_column(pred_col, F.lit(9999))\n    df.write.save_as_table(input_table_name, mode='overwrite')\n    update_sql1 = f\"\"\"\n    UPDATE {input_table_name} t\n    SET {pred_col} = r.{pred_col}\n    FROM DEMO_TEMP_PREDICTION_RESULTS r\n    WHERE t.CUSTOMER_ID = r.CUSTOMER_ID\n    AND t.TRANSACTION_TIME=r.TRANSACTION_TIME ;\n    \"\"\"\n    \n    # Execute the update statement\n    session.sql(update_sql1).collect()\n\n    return \"Success\"\n\n# Register the stored procedure\nsession.sproc.register(\n    func=demo_inference_sproc,\n    name=\"fraud_prediction_inference_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@AI_ML_STAGE\",\n    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=StringType()\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdb405ce-4a47-4ebc-b176-0d1401b5b928",
   "metadata": {
    "language": "sql",
    "name": "cal_the_prediction"
   },
   "outputs": [],
   "source": "CALL fraud_prediction_inference_sproc('DEMO_FRAUD_TRAIN','FRAUD_ANALYSIS_XGB', 'V2');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d7fe70a-af77-4e19-87eb-131686c4c6d4",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "SELECT * FROM DEMO_FRAUD_TRAIN LIMIT 2; ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a50664c6-158d-49ed-9e0e-d9043b20d34f",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "CALL fraud_prediction_inference_sproc('DEMO_FRAUD_TEST','FRAUD_ANALYSIS_XGB', 'V2');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e16a05e7-d52b-4d9b-a952-2748c004909e",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "SELECT * FROM DEMO_FRAUD_TEST LIMIT 2; ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8cc6cff0-4091-49d4-bab4-fb05a53965cf",
   "metadata": {
    "language": "sql",
    "name": "monitor_creation"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR FRAUD_ANALYSIS_XGB_MODEL_MONITOR\nWITH\n    MODEL=FRAUD_ANALYSIS_XGB\n    VERSION=V1\n    FUNCTION=predict\n    SOURCE=DEMO_FRAUD_TEST\n    BASELINE=DEMO_FRAUD_TRAIN\n    TIMESTAMP_COLUMN=TRANSACTION_TIME\n    PREDICTION_CLASS_COLUMNS=(V1_PREDICTION)  \n    ACTUAL_CLASS_COLUMNS=(IS_FRAUD)\n    ID_COLUMNS=(CUSTOMER_ID)\n    WAREHOUSE=COMPUTE_WH\n    REFRESH_INTERVAL='5 min'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cae6992-df23-439c-a846-0173db998fa1",
   "metadata": {
    "language": "sql",
    "name": "show_model_monitor"
   },
   "outputs": [],
   "source": "SHOW MODEL MONITORS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd0b8bc0-fca5-4f3d-98f1-53bc4d2d213b",
   "metadata": {
    "name": "model_drift",
    "collapsed": false
   },
   "source": "Valid Time Units 1 DAY 1 WEEK 1 MONTH 1 QUARTER 1 YEAR\n\nCustom Time Periods\nThe amount must be a positive integer, allowing for custom aggregation periods such as:\n\n7 DAY\n14 DAY\n3 MONTH"
  },
  {
   "cell_type": "code",
   "id": "130ea583-3096-40c8-9987-d480b7888d23",
   "metadata": {
    "language": "sql",
    "name": "model_drift_amount"
   },
   "outputs": [],
   "source": "SELECT *\nFROM TABLE(MODEL_MONITOR_DRIFT_METRIC(\n    'FRAUD_ANALYSIS_XGB_MODEL_MONITOR', \n    'JENSEN_SHANNON',                   \n    'TRANSACTION_AMOUNT',                \n    '1 DAY',                             \n))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d91ab7ed-75c5-4931-91fb-321740bd1036",
   "metadata": {
    "language": "sql",
    "name": "model_performance_metrics"
   },
   "outputs": [],
   "source": "SELECT *\nFROM TABLE(MODEL_MONITOR_PERFORMANCE_METRIC(\n    'FRAUD_ANALYSIS_XGB_MODEL_MONITOR',     -- model_monitor_name\n    'CLASSIFICATION_ACCURACY',              -- updated metric_name\n    '1 DAY',                                -- granularity\n    DATEADD(day, -30, CURRENT_DATE()),      -- start_time (30 days ago)\n    CURRENT_DATE()                          -- end_time (today)\n))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "143b339a-1ddf-4853-a408-d6545b58b1ac",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": "-- SELECT *\n-- FROM TABLE(MODEL_MONITOR_STAT_METRIC(\n--     'FRAUD_ANALYSIS_XGB_MODEL_MONITOR',     -- model_monitor_name\n--     CLASSIFICATION_ACCURACY',              -- updated metric_name\n--     '1 DAY',                                -- granularity\n--     DATEADD(day, -30, CURRENT_DATE()),      -- start_time (30 days ago)\n--     CURRENT_DATE()                          -- end_time (today)\n-- ))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05b908f9-4f53-46a5-a37c-e4b7362a88b2",
   "metadata": {
    "name": "end",
    "collapsed": false
   },
   "source": "              ******************************** End of notebook **********************"
  }
 ]
}